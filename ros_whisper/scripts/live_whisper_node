#!/usr/bin/env python
import sys
from collections import deque

import rospy

import threading

import numpy as np
import torch
import faster_whisper

import time

from audio_common_msgs.msg import AudioData, AudioInfo
from std_msgs.msg import String

from ros_whisper.live_whisper import LiveWhisper, OnlineASRProcessor


class LiveWhisperNode:
    def __init__(self, model_or_path, language="en", compute_type="default"):

        self.pub = rospy.Publisher("~text", String, queue_size=1)
        self.max_length = 8.0

        rospy.logdebug(logger_name="Whisper", msg=f"waiting for audio_info...")
        audio_info: AudioInfo = rospy.wait_for_message(
            "audio_info", AudioInfo, timeout=5
        )

        if not audio_info.sample_rate == 16000:
            msg = f"Sample rate needs to be 16000"
            rospy.logerr(logger_name="Whisper", msg=msg)
            raise msg

        if not audio_info.channels == 1:
            msg = f"Cant use non mono audio"
            rospy.logerr(logger_name="Whisper", msg=msg)
            raise msg

        if audio_info.sample_format.lower() == "s16le":
            self.depth_type = np.int16
            self.nomalization = float(np.iinfo(self.depth_type).max)
        elif audio_info.sample_format.lower() == "f32le":
            self.depth_type = np.float32
            self.nomalization = None
        else:
            msg = f"unhandled audio format"
            rospy.logerr(logger_name="Whisper", msg=msg)
            raise msg

        self.buffer_lock = threading.Lock()
        self.buffer = np.array([], dtype=np.float32)

        asr = LiveWhisper(model_or_path, language, compute_type)
        # warm up the ASR because the very first transcribe takes much more time than the other
        rospy.logdebug("warmup...")
        sampl = np.random.uniform(low=0.0, high=1, size=(1000,))
        asr.transcribe(sampl)
        self.processor = OnlineASRProcessor(asr)
        rospy.logdebug("Whisper initialized")

        rospy.Subscriber("~input", AudioData, self.audio_cb)

        self.num = 0
        rospy.Timer(rospy.Duration(1.0), self.timer)

    def audio_cb(self, data):
        value = np.frombuffer(data.data, dtype=self.depth_type)
        self.processor.insert_audio_chunk(value)

    def timer(self, event=None):
        rospy.loginfo(f"starting transcription..")

        ts = time.time()
        o = self.processor.process_iter()
        output_transcript(o)
        te = time.time()
        rospy.loginfo(f"whisper took: {te - ts}")


start = time.time()


def output_transcript(o, now=None):
    # output format in stdout is like:
    # 4186.3606 0 1720 Takhle to je
    # - the first three words are:
    #    - emission time from beginning of processing, in milliseconds
    #    - beg and end timestamp of the text segment, as estimated by Whisper model. The timestamps are not accurate, but they're useful anyway
    # - the next words: segment transcript
    if now is None:
        now = time.time() - start
    if o[0] is not None:
        print(
            "%1.4f %1.0f %1.0f %s" % (now * 1000, o[0] * 1000, o[1] * 1000, o[2]),
            file=sys.stderr,
            flush=True,
        )
        print(
            "%1.4f %1.0f %1.0f %s" % (now * 1000, o[0] * 1000, o[1] * 1000, o[2]),
            flush=True,
        )
    else:
        # No text, so no output
        pass


if __name__ == "__main__":

    # Start ROS node
    rospy.init_node("ros_whisper_transcribe", log_level=rospy.DEBUG)
    model_or_path = rospy.get_param("~model_or_path", "base.en")
    language = rospy.get_param("~language", "en")
    quant = rospy.get_param("~quant", "default")
    rospy.loginfo(f"using model: {model_or_path}")

    node = LiveWhisperNode(model_or_path, language, quant)

    rospy.loginfo("Whisper node running")

    rospy.spin()
